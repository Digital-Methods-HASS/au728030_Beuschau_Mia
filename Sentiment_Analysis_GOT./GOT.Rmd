---
title: "Game_of_thrones"
author: "Mia Juul Beuschau"
date: "2023-10-10"
output: html_document
---

## Loading tools

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```

## Loading the got pdf to this rmarkdown

```{r load data}
got_path <- here("data","got.pdf")
got_text <- pdf_text(got_path)
```

## Looking at side 502

Example: Just want to get text from a single page 502? 
```{r single-page}

got_textf <- got_text[502]

got_textf


```
now you can see that it have added \n every time there is a new line


## Some wrangling (split lines, removing stopwords, wm.):


```{r split-lines}
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 


```



```{r split by words}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
got_tokens
```

Let's count the words!
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```


### Remove stop words:

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)



```
Now check the counts again: 
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
```



### removing the numbers(non-text) in `got_stop`
```{r skip-numbers}

ipcc_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))

ipcc_no_numeric
```


## A word cloud of GOT report words (non-numeric)

```{r wordcloud-prep}
# There are almost 2000 unique words 
length(unique(ipcc_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
got_top100 <- ipcc_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)

```



```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud
```






That's underwhelming. Let's customize it a bit:
```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```



## USING the three general-purpose lexicons

they are
  -  AFINN from Finn Ã…rup Nielsen,
  -  bing from Bing Liu and collaborators, and
  -  nrc from Saif Mohammad and Peter Turney




### "afinn": Words ranked from -5 (very negative) to +5 (very positive)
```{r afinn, loading feelingwords}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))
afinn_pos
```




### Bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```





*Citation for NRC lexicon**: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.



### Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```





# Sentiment analysis with afinn: 

First, bind words in `ipcc_stop` to `afinn` lexicon:
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
```

## Counts by sentiment rankings:
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```

there are more negative than positive words



## Investigation of the words with the value 4:
```{r afinn-4}
# What are these '4' words?
got_afinn4 <- got_afinn %>% 
  filter(value == 4)
got_afinn4
```




```{r afinn-4-more}
# Check the unique 4-score words:
unique(got_afinn4$word)

# Count & plot them
got_afinn4_n <- got_afinn4 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn4_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

```





Or we can summarize sentiment for the report: 
```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
got_summary

```

### The mean of the value of the words is negative. 

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 

```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```


### We check which words are excluded using `anti_join()`:

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(ipcc_exclude)

# the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```


## Counts of the feeling: 
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```





## Overview of which of the words occurs the most in each sentiment



Or count by sentiment *and* word, then facet:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("figures","got_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```




## We will check why "lord" appears so many times
```{r nrc-confidence}
lord <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "lord")

# Yep, check it out:
lord
```
















